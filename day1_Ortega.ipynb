{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "day1-Ortega.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/maxdelvecchyo/Deep-Learning/blob/master/day1_Ortega.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ACEwSn1tj0iT",
        "colab_type": "text"
      },
      "source": [
        "# Set up CoLab environment: Install spark and configure python to work with it"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U-1f_pOej05k",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 216
        },
        "outputId": "cb229166-bcba-4a03-92a3-9a3ecd1c793a"
      },
      "source": [
        "!pip install pyspark\n",
        "!pip install -U -q PyDrive\n",
        "!apt install openjdk-8-jdk-headless -qq\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "\n",
        "import pyspark\n",
        "from pyspark.sql import *\n",
        "from pyspark.sql.types import *\n",
        "from pyspark.sql.functions import *\n",
        "from pyspark import SparkContext, SparkConf\n",
        "from IPython.display import clear_output\n",
        "\n",
        "# create the session\n",
        "conf = SparkConf().set(\"spark.ui.port\", \"4050\")\n",
        "\n",
        "# create the context\n",
        "# sc = pyspark.SparkContext(conf=conf)\n",
        "spark = SparkSession.builder.config(\"spark.ui.port\",\"4050\").getOrCreate()\n",
        "clear_output()\n",
        "spark"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "            <div>\n",
              "                <p><b>SparkSession - in-memory</b></p>\n",
              "                \n",
              "        <div>\n",
              "            <p><b>SparkContext</b></p>\n",
              "\n",
              "            <p><a href=\"http://b36ee1ff1f5a:4050\">Spark UI</a></p>\n",
              "\n",
              "            <dl>\n",
              "              <dt>Version</dt>\n",
              "                <dd><code>v3.0.0</code></dd>\n",
              "              <dt>Master</dt>\n",
              "                <dd><code>local[*]</code></dd>\n",
              "              <dt>AppName</dt>\n",
              "                <dd><code>pyspark-shell</code></dd>\n",
              "            </dl>\n",
              "        </div>\n",
              "        \n",
              "            </div>\n",
              "        "
            ],
            "text/plain": [
              "<pyspark.sql.session.SparkSession at 0x7f4b3085a080>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q1Q8Ryt1hZCT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "507706f7-f958-4a5d-92e2-13957a8caca1"
      },
      "source": [
        "# As you will notice, Spark UI does not work. So please run this cell.\n",
        "!wget https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip\n",
        "!unzip ngrok-stable-linux-amd64.zip\n",
        "get_ipython().system_raw('./ngrok http 4050 &')\n",
        "clear_output()\n",
        "!curl -s http://localhost:4040/api/tunnels | python3 -c \\\n",
        "    \"import sys, json; print(json.load(sys.stdin)['tunnels'][0]['public_url'])\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "http://ec89e69fa4fe.ngrok.io\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zLGJy5W0thqA",
        "colab_type": "text"
      },
      "source": [
        "## Move the kaggle dataset from drive to Colab file system\n",
        "Run cell"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Svhzy-y98Jz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "0232e45f-4a36-46e4-8c94-36e4e0f1dd12"
      },
      "source": [
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Download files from Vantage Drive to local colab drive!\n",
        "!cp /content/drive/Shared\\ drives/Vantage\\ AI/Kennismanagement/Vantage\\ Program/2020-07-12\\ Spark\\ Guido\\ +\\ Joris/Data\\ Handson/data-handson.zip /content/\n",
        "!unzip /content/data-handson.zip -d /content/data/\n",
        "!rm /content/data-handson.zip\n",
        "!rm -rf /content/sample_data/\n",
        "\n",
        "clear_output()\n",
        "print('Setup is all done! Back to class')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Setup is all done! Back to class\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-nht298ukwB1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0LlrWGzDtzxr",
        "colab_type": "text"
      },
      "source": [
        "# Hands-on - DAY 1\n",
        "\n",
        "## Load in the data files (ETL)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1X3OuvoGieZL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "import pyspark\n",
        "from pyspark.sql import *\n",
        "from pyspark.sql.types import *\n",
        "from pyspark.sql.functions import *"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XuijKtXVBHxo",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "bdaadcb9-7e18-4bef-8312-57f9b8480adc"
      },
      "source": [
        "aisles = spark.read.format('csv').load('data/aisles.csv')\n",
        "aisles.show(5)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+--------+--------------------+\n",
            "|     _c0|                 _c1|\n",
            "+--------+--------------------+\n",
            "|aisle_id|               aisle|\n",
            "|       1|prepared soups sa...|\n",
            "|       2|   specialty cheeses|\n",
            "|       3| energy granola bars|\n",
            "|       4|       instant foods|\n",
            "+--------+--------------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nFVjahggBkqZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "a1510692-b7fc-4566-db61-a06dfafa855a"
      },
      "source": [
        "# Indicate that the data has a header\n",
        "aisles = spark.read.format('csv').option('header','true').load('data/aisles.csv')\n",
        "aisles.show(5)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+--------+--------------------+\n",
            "|aisle_id|               aisle|\n",
            "+--------+--------------------+\n",
            "|       1|prepared soups sa...|\n",
            "|       2|   specialty cheeses|\n",
            "|       3| energy granola bars|\n",
            "|       4|       instant foods|\n",
            "|       5|marinades meat pr...|\n",
            "+--------+--------------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jaF3n3EzBkse",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        },
        "outputId": "29403397-9f60-448c-c667-89dd72d91aab"
      },
      "source": [
        "# Or with less code\n",
        "aisles = spark.read.csv('data/aisles.csv', header=True)\n",
        "aisles.show(5)\n",
        "print('Schema: ')\n",
        "aisles.printSchema()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+--------+--------------------+\n",
            "|aisle_id|               aisle|\n",
            "+--------+--------------------+\n",
            "|       1|prepared soups sa...|\n",
            "|       2|   specialty cheeses|\n",
            "|       3| energy granola bars|\n",
            "|       4|       instant foods|\n",
            "|       5|marinades meat pr...|\n",
            "+--------+--------------------+\n",
            "only showing top 5 rows\n",
            "\n",
            "Schema: \n",
            "root\n",
            " |-- aisle_id: string (nullable = true)\n",
            " |-- aisle: string (nullable = true)\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8a8SrYusByNd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "outputId": "fb6ad643-0d14-43b3-a509-bfda95c909ae"
      },
      "source": [
        "from pyspark.sql import functions as F\n",
        "# Cast columns to a typ\n",
        "aisles.withColumn('aisle_id',F.col('aisle_id').cast('integer')).printSchema()\n",
        "aisles.selectExpr('cast(aisle_id as int) as aisle_id','aisle').printSchema()\n",
        "spark.read.csv('data/aisles.csv', header=True, schema='aisle_id int, aisle string', mode='FAILFAST').printSchema()\n",
        "# NOTE, these lines of code are all lazy, so nothing is actually loaded."
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "root\n",
            " |-- aisle_id: integer (nullable = true)\n",
            " |-- aisle: string (nullable = true)\n",
            "\n",
            "root\n",
            " |-- aisle_id: integer (nullable = true)\n",
            " |-- aisle: string (nullable = true)\n",
            "\n",
            "root\n",
            " |-- aisle_id: integer (nullable = true)\n",
            " |-- aisle: string (nullable = true)\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CFYwrmTat_df",
        "colab_type": "text"
      },
      "source": [
        "## 1. Load in `aisles.csv`, cast data to correct types and write to parquet:\n",
        "\n",
        "`df.write.format('parquet').save('<folder-name>')`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "12u5l1pouM7k",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 324
        },
        "outputId": "2847ffd6-a33a-4c09-ca2c-098e8a4fdec8"
      },
      "source": [
        "aisles = spark.read.csv('data/aisles.csv', header=True)\n",
        "aisles = aisles.withColumn('aisle_id',F.col('aisle_id').cast('integer'))\n",
        "aisles.printSchema()\n",
        "aisles.write.format('parquet').save('./data/aisles')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "root\n",
            " |-- aisle_id: integer (nullable = true)\n",
            " |-- aisle: string (nullable = true)\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "AnalysisException",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-cc4165dbc6f5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0maisles\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maisles\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwithColumn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'aisle_id'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'aisle_id'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'integer'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0maisles\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprintSchema\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0maisles\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'parquet'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./data/aisles'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pyspark/sql/readwriter.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self, path, format, mode, partitionBy, **options)\u001b[0m\n\u001b[1;32m    825\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    826\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 827\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    828\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    829\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0msince\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1.4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1304\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1305\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1307\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    135\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 137\u001b[0;31m                 \u001b[0mraise_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconverted\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    138\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mraise_from\u001b[0;34m(e)\u001b[0m\n",
            "\u001b[0;31mAnalysisException\u001b[0m: path file:/content/data/aisles already exists.;"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "PL9QgCx4C7KE"
      },
      "source": [
        "## 2. Load in `departments.csv`, cast data to correct types and write to parquet\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "LzG4DGPcC7nK",
        "colab": {}
      },
      "source": [
        "departments = spark.read.csv('data/departments.csv', header=True)\n",
        "departments = departments.withColumn('department_id',F.col('department_id').cast('integer'))\n",
        "departments.write.format('parquet').save('./data/departments')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rbu5mAtUDJvD",
        "colab_type": "text"
      },
      "source": [
        "## 3. Load in `orders.csv`, cast data to correct types and write to spark storage:\n",
        "`df.write.mode('overwrite').saveAsTable('<table-name>') `\n",
        "\n",
        "Now you can also query the data with sql! \n",
        "\n",
        "df2 = `spark.sql('SELECT * FROM <table-name>')`\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0aYleXt0FPW4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        },
        "outputId": "0ea1db0a-77fc-4a3d-da5e-119ccc0814fc"
      },
      "source": [
        " # Noob alert\n",
        " orders = spark.read.format('json').load('./data/orders.txt')\n",
        " orders = orders.withColumn('days_since_prior_order',F.col('days_since_prior_order').cast('integer'))\n",
        " orders = orders.withColumn('order_dow',F.col('order_dow').cast('integer'))\n",
        " orders = orders.withColumn('order_hour_of_day',F.col('order_hour_of_day').cast('integer'))\n",
        " orders = orders.withColumn('order_id',F.col('order_id').cast('integer'))\n",
        " orders = orders.withColumn('order_number',F.col('order_number').cast('integer'))\n",
        " orders = orders.withColumn('user_id',F.col('user_id').cast('integer'))\n",
        " orders.printSchema()\n",
        " orders.show(5)\n",
        " orders.write.saveAsTable('orders')\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "root\n",
            " |-- days_since_prior_order: integer (nullable = true)\n",
            " |-- eval_set: string (nullable = true)\n",
            " |-- order_dow: integer (nullable = true)\n",
            " |-- order_hour_of_day: integer (nullable = true)\n",
            " |-- order_id: integer (nullable = true)\n",
            " |-- order_number: integer (nullable = true)\n",
            " |-- user_id: integer (nullable = true)\n",
            "\n",
            "+----------------------+--------+---------+-----------------+--------+------------+-------+\n",
            "|days_since_prior_order|eval_set|order_dow|order_hour_of_day|order_id|order_number|user_id|\n",
            "+----------------------+--------+---------+-----------------+--------+------------+-------+\n",
            "|                  null| history|        6|               12| 2618231|           0|     13|\n",
            "|                     8| history|        0|               11| 2560699|           1|     13|\n",
            "|                     6| history|        6|               21| 2288946|           2|     13|\n",
            "|                     9| history|        1|               12|   19256|           3|     13|\n",
            "|                     6| history|        0|               13| 1378982|           4|     13|\n",
            "+----------------------+--------+---------+-----------------+--------+------------+-------+\n",
            "only showing top 5 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xItTQnK3EJXp",
        "colab_type": "text"
      },
      "source": [
        "## 4. Load in `orderContents`, set the right column names and save the table\n",
        "\n",
        "Maybe register the resulting dataframe as a temporary sql table.\n",
        "\n",
        "`df.createOrReplaceTempView('<table-name')`\n",
        "\n",
        "Now it is also possible to query this table!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p3aT10lQD3WJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 596
        },
        "outputId": "c2df376c-0078-437e-9a54-e7b35525d59e"
      },
      "source": [
        "ordercontents = spark.read.parquet('./data/orderContents')\n",
        "ordercontents = ordercontents.withColumnRenamed(\"_c0\",\"order_id\")\n",
        "ordercontents = ordercontents.withColumnRenamed(\"_c1\",\"product_id\")\n",
        "ordercontents = ordercontents.withColumnRenamed(\"_c2\",\"add_to_cart_order\")\n",
        "ordercontents = ordercontents.withColumnRenamed(\"_c3\",\"reordered\")\n",
        "ordercontents.show(5)\n",
        "orders.printSchema()\n",
        "\n",
        "ordercontents.write.saveAsTable('ordercontents')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+--------+----------+-----------------+---------+\n",
            "|order_id|product_id|add_to_cart_order|reordered|\n",
            "+--------+----------+-----------------+---------+\n",
            "|  161263|     18376|                4|        1|\n",
            "|   36981|     17024|                4|        0|\n",
            "|   86528|     35752|                4|        1|\n",
            "|  575540|     33956|                3|        0|\n",
            "|  185986|     21637|                6|        0|\n",
            "+--------+----------+-----------------+---------+\n",
            "only showing top 5 rows\n",
            "\n",
            "root\n",
            " |-- days_since_prior_order: integer (nullable = true)\n",
            " |-- eval_set: string (nullable = true)\n",
            " |-- order_dow: integer (nullable = true)\n",
            " |-- order_hour_of_day: integer (nullable = true)\n",
            " |-- order_id: integer (nullable = true)\n",
            " |-- order_number: integer (nullable = true)\n",
            " |-- user_id: integer (nullable = true)\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "AnalysisException",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-36-0836e86a0114>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0morders\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprintSchema\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mordercontents\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msaveAsTable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'ordercontents'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pyspark/sql/readwriter.py\u001b[0m in \u001b[0;36msaveAsTable\u001b[0;34m(self, name, format, mode, partitionBy, **options)\u001b[0m\n\u001b[1;32m    866\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mformat\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    867\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 868\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msaveAsTable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    869\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    870\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0msince\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1.4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1304\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1305\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1307\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    135\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 137\u001b[0;31m                 \u001b[0mraise_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconverted\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    138\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mraise_from\u001b[0;34m(e)\u001b[0m\n",
            "\u001b[0;31mAnalysisException\u001b[0m: Table `ordercontents` already exists.;"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RxD6S1aHEojJ",
        "colab_type": "text"
      },
      "source": [
        "## 5. Find out datatype of products.txt, load the data, cast types and save (in a format keeping the types you just set)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D6M3vQDhE2lg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 306
        },
        "outputId": "8af3abc3-0562-4a47-fcd2-e8fec87f9e9a"
      },
      "source": [
        "products = spark.read.csv('data/products.csv', header=True)\n",
        "products = products.withColumn('product_id',F.col('product_id').cast('integer'))\n",
        "products = products.withColumn('aisle_id',F.col('aisle_id').cast('integer'))\n",
        "products = products.withColumn('department_id',F.col('department_id').cast('integer'))\n",
        "products.printSchema()\n",
        "products.show(5)\n",
        "products.write.format('parquet').save('./data/products')\n",
        "#products = departments.withColumn('department_id',F.col('department_id').cast('integer'))\n",
        "#departments.write.format('parquet').save('./data/departments')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "root\n",
            " |-- product_id: integer (nullable = true)\n",
            " |-- product_name: string (nullable = true)\n",
            " |-- aisle_id: integer (nullable = true)\n",
            " |-- department_id: integer (nullable = true)\n",
            "\n",
            "+----------+--------------------+--------+-------------+\n",
            "|product_id|        product_name|aisle_id|department_id|\n",
            "+----------+--------------------+--------+-------------+\n",
            "|         1|Chocolate Sandwic...|      61|           19|\n",
            "|         2|    All-Seasons Salt|     104|           13|\n",
            "|         3|Robust Golden Uns...|      94|            7|\n",
            "|         4|Smart Ones Classi...|      38|            1|\n",
            "|         5|Green Chile Anyti...|       5|           13|\n",
            "+----------+--------------------+--------+-------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "c4OkMa6RC7nJ"
      },
      "source": [
        "# Label Engineering\n",
        "Goal: manipulate the data in such a way that we obtain labels.\n",
        "\n",
        "|user_id|product_id|product_bought_in_last_visit|\n",
        "|---|---|---|\n",
        "| A | 1 | 1 |\n",
        "|A  | 5  |0  |\n",
        "| ... | ... | ... |\n",
        "| B | 2 | 0 |\n",
        "|B| 3| 1|\n",
        "|...|...|...|\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ccqOJI2fC7KF",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BYRsIT9fuR7P",
        "colab_type": "text"
      },
      "source": [
        "# Feature Engineering\n",
        "Goal: Create both user and product features\n",
        "\n",
        "## User features:\n",
        "1. Average basket size\n",
        "2. Whether the user ordered organic, \u000bgluten-free, or Asian items in the past\n",
        "3. Number of distinct items\n",
        "4. Average time between orders"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ay9RvsdMuS43",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jBsCE8wIIRA4",
        "colab_type": "text"
      },
      "source": [
        "## Product Features\n",
        "1. Probability of a product being bought on a certain day\n",
        "2. Average time between buys\n",
        "3. Number of times being bought\n",
        "4. Probability of an item being reordered?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7-s4-dAAISB1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BZuh2lRyIVQE",
        "colab_type": "text"
      },
      "source": [
        "## UserxProduct Features\n",
        "1. How often has the user bought an item in the past\n",
        "2. Average time for a given user between being a product"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XEDQSaqJIY5k",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Yz8t_PvI0OX",
        "colab_type": "text"
      },
      "source": [
        "#  DAY 2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gOqEshVwIz4B",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}